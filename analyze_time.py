import pandas as pd
import psycopg2
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
from datetime import datetime, date

DB_CONFIG = { 
    'user': 'postgres', 
    'password': 'Quan@', 
    'host': 'localhost', 
    'port': '5432', 
    'database': 'flood_alert_db' 
}

def convert_time_to_seconds(time_str):
    """
    Chuyá»ƒn Ä‘á»•i chuá»—i thá»i gian 'X phÃºt Y giÃ¢y' thÃ nh sá»‘ giÃ¢y
    """
    if pd.isna(time_str) or time_str == '' or time_str is None:
        return 0.0
    
    try:
        return float(time_str)
    except:
        pass
    
    try:
        if 'phÃºt' in str(time_str) and 'giÃ¢y' in str(time_str):
            parts = str(time_str).split()
            minutes = 0
            seconds = 0
            
            for i in range(len(parts)):
                if parts[i] == 'phÃºt':
                    minutes = float(parts[i-1])
                elif parts[i] == 'giÃ¢y':
                    seconds = float(parts[i-1])
            
            return minutes * 60 + seconds
        elif 'giÃ¢y' in str(time_str):
            parts = str(time_str).split()
            for i in range(len(parts)):
                if parts[i] == 'giÃ¢y':
                    return float(parts[i-1])
        else:
            return float(time_str)
    except:
        return 0.0

def create_features_from_db(df):
    """
    Táº¡o Ä‘áº·c trÆ°ng NÃ‚NG CAO cho dá»± Ä‘oÃ¡n thá»i gian
    """
    print("ğŸ”„ Äang táº¡o Ä‘áº·c trÆ°ng NÃ‚NG CAO cho dá»± Ä‘oÃ¡n thá»i gian...")
    df = df.sort_values(by='created_at')

    # ğŸ†š CHUYá»‚N Äá»”I Sá»
    df['mucnuoca'] = pd.to_numeric(df['mucnuoca'], errors='coerce').fillna(0)
    df['mucnuocb'] = pd.to_numeric(df['mucnuocb'], errors='coerce').fillna(0)
    df['luuluong'] = pd.to_numeric(df['luuluong'], errors='coerce').fillna(0)
    
    # ğŸ¯ QUAN TRá»ŒNG: Chuyá»ƒn Ä‘á»•i time_until_a_danger
    print("   ğŸ”„ Chuyá»ƒn Ä‘á»•i time_until_a_danger tá»« string sang sá»‘ giÃ¢y...")
    df['time_until_a_danger_seconds'] = df['time_until_a_danger'].apply(convert_time_to_seconds)
    
    # ğŸ“Š TÃNH TOÃN Äáº¶C TRÆ¯NG NÃ‚NG CAO
    df['time_diff'] = df['created_at'].diff().dt.total_seconds().fillna(0)
    
    # Tá»‘c Ä‘á»™ thay Ä‘á»•i
    df['b_rate_of_change'] = df['mucnuocb'].diff() / df['time_diff']
    df['flow_rate_of_change'] = df['luuluong'].diff() / df['time_diff']
    
    # ChÃªnh lá»‡ch vÃ  tá»· lá»‡
    df['ab_diff'] = df['mucnuocb'] - df['mucnuoca']
    df['ab_ratio'] = df['mucnuocb'] / (df['mucnuoca'] + 0.001)
    
    # Thay Ä‘á»•i tuyá»‡t Ä‘á»‘i
    df['b_absolute_change'] = df['mucnuocb'].diff().fillna(0)
    df['b_total_rise'] = df['mucnuocb'] - df['mucnuocb'].iloc[0]
    
    # Chá»‰ sá»‘ nguy hiá»ƒm tá»•ng há»£p
    df['danger_index'] = (
        (df['mucnuocb'] * 0.3) + 
        (df['b_rate_of_change'].abs() * 2.0) + 
        (df['b_absolute_change'].abs() * 0.5) +
        (df['ab_diff'] * 0.2)
    )
    
    # Xu hÆ°á»›ng
    df['b_trend'] = df['mucnuocb'].rolling(window=3, min_periods=1).mean()
    
    # ğŸ¯ DÃ™NG Dá»® LIá»†U is_raining THá»°C Táº¾
    df['is_raining_now'] = df['is_raining'].astype(int)
    
    df = df.replace([np.inf, -np.inf], 0).fillna(0)
    
    print(f"   ğŸ“Š Target range: {df['time_until_a_danger_seconds'].min():.1f} - {df['time_until_a_danger_seconds'].max():.1f} giÃ¢y")
    print(f"   ğŸ“Š Má»±c nÆ°á»›c B: {df['mucnuocb'].min():.1f} - {df['mucnuocb'].max():.1f} cm")
    
    return df

print("ğŸŒ Äang káº¿t ná»‘i tá»›i PostgreSQL...")
try:
    conn = psycopg2.connect(**DB_CONFIG)
    
    # ğŸ¯ CHá»ˆ Láº¤Y Dá»® LIá»†U NGÃ€Y HÃ”M NAY
    today = date.today().strftime('%Y-%m-%d')
    query = f"SELECT * FROM public.sensor_data WHERE DATE(created_at) = '{today}';"
    
    df = pd.read_sql_query(query, conn)
    conn.close()

    print(f"âœ… Láº¥y dá»¯ liá»‡u NGÃ€Y {today} thÃ nh cÃ´ng! Tá»•ng cá»™ng {len(df)} hÃ ng.")

    if len(df) < 5:
        print(f"âŒ QuÃ¡ Ã­t dá»¯ liá»‡u ({len(df)} hÃ ng). Cáº§n Ã­t nháº¥t 5 hÃ ng Ä‘á»ƒ huáº¥n luyá»‡n.")
        exit()

    # Táº¡o features
    df_features = create_features_from_db(df)
    
    features = [
        'mucnuoca', 'mucnuocb', 'luuluong', 'is_raining_now',
        'b_rate_of_change', 'flow_rate_of_change', 'ab_diff', 
        'ab_ratio', 'b_absolute_change', 'b_total_rise', 'danger_index', 'b_trend'
    ]
    
    X = df_features[features]
    y = df_features['time_until_a_danger_seconds']

    print("ğŸ¯ Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh Äáº¾M NGÆ¯á»¢C NÃ‚NG CAO...")
    print(f"   ğŸ“Š Target range: {y.min():.1f} - {y.max():.1f} giÃ¢y")
    
    # Äiá»u chá»‰nh cho Ã­t dá»¯ liá»‡u
    if len(df) > 10:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    else:
        X_train, y_train = X, y
        X_test, y_test = X, y
        print("âš ï¸ DÃ¹ng toÃ n bá»™ dá»¯ liá»‡u cho training (quÃ¡ Ã­t máº«u)")
    
    # Äiá»u chá»‰nh model cho dá»¯ liá»‡u Ã­t
    n_estimators = min(50, len(X_train) // 2)
    max_depth = min(8, max(3, len(X_train) // 3))
    
    model = RandomForestRegressor(
        n_estimators=max(10, n_estimators),
        max_depth=max_depth,
        random_state=42,
        min_samples_split=2,
        min_samples_leaf=1
    )
    
    model.fit(X_train, y_train)
    print("âœ… Huáº¥n luyá»‡n hoÃ n táº¥t!")
    
    # ÄÃ¡nh giÃ¡ model
    predictions = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    r2 = r2_score(y_test, predictions)
    
    print("\nğŸ“ˆ Káº¾T QUáº¢ MODEL Äáº¾M NGÆ¯á»¢C NÃ‚NG CAO:")
    print(f"   âœ… RMSE: {rmse:.2f} giÃ¢y")
    print(f"   âœ… RÂ² Score: {r2:.3f}")
    print(f"   ğŸ“Š Sá»‘ máº«u training: {len(X_train)}")
    print(f"   ğŸ“Š Sá»‘ máº«u testing: {len(X_test)}")
    
    # PhÃ¢n tÃ­ch feature importance
    feature_importance = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ” Feature Importance:")
    for _, row in feature_importance.head(5).iterrows():
        print(f"   {row['feature']}: {row['importance']:.3f}")
    
    # Dá»± Ä‘oÃ¡n thá»­
    print("\nğŸ¯ Dá»° ÄOÃN THá»¬ TRÃŠN 5 MáºªU Gáº¦N ÄÃ‚Y:")
    recent_data = X.tail(5)
    recent_predictions = model.predict(recent_data)
    
    for i, (actual, pred) in enumerate(zip(y.tail(5).values, recent_predictions)):
        print(f"   Máº«u {i+1}: Thá»±c táº¿ {actual:.1f}s -> Dá»± Ä‘oÃ¡n {pred:.1f}s")
    
    # LÆ°u model
    model_filename = f'time_model_today.pkl'
    joblib.dump(model, model_filename)
    print(f"ğŸ’¾ ÄÃ£ lÆ°u model Ä‘áº¿m ngÆ°á»£c NÃ‚NG CAO vÃ o '{model_filename}'")
    
except Exception as e:
    print(f"âŒ Lá»—i: {e}")
    import traceback
    traceback.print_exc()